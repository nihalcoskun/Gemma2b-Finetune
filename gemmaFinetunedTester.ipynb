{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HvmtLrEbpKb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6CgFCheb3YP"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIYJXXKnwUMm"
      },
      "outputs": [],
      "source": [
        "#single generation\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Model ve tokenizer yollarını belirtin\n",
        "model_path = \"/content/sample_data/models/\"\n",
        "tokenizer_path = \"/content/sample_data/models/\"\n",
        "\n",
        "# Tokenizer ve modeli yerel yoldan yükleme\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Modeli direkt olarak kullanın (herhangi bir .to veya .half çağrısı yapmadan)\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Verilen cümlenin orijinal anlamı koruyup, farklı kelimeler kullanarak yeniden ifade et.\", # instruction\n",
        "            \"bir sosyal hizmet uzmanı eski bir müşteriyle buluşabilir mi?\", # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "# Modeli kullanarak token üretimi\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Çıktıları decode et\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC4IVtWt-lAv",
        "outputId": "e66d2735-7b7a-4e59-c12e-2b2c5588a4d4"
      },
      "outputs": [],
      "source": [
        "#excel test generation\n",
        "\n",
        "from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import openpyxl\n",
        "\n",
        "\n",
        "# Model ve tokenizer yollarını belirtin\n",
        "model_path = \"/content/sample_data/models/\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "# Excel dosyasını yükleyin\n",
        "wb = openpyxl.load_workbook('exceltest600.xlsx')\n",
        "sheet = wb.active\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "# Excel dosyasındaki verileri kullanarak modeli çalıştırın\n",
        "for row in range(2, sheet.max_row + 1):\n",
        "    instruction = \"Verilen cümlenin orijinal anlamı koruyup, farklı kelimeler kullanarak yeniden ifade et.\"\n",
        "    input_text = sheet[f'A{row}'].value\n",
        "\n",
        "    # Model için girdileri hazırlayın\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(instruction, input_text, \"\")\n",
        "        ],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "# Modeli çalıştırın ve sonuçları alın\n",
        "    outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n",
        "\n",
        "    # Sonuçları decode edin ve Excel dosyasına yazın\n",
        "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # İlk olarak, sadece modelin ürettiği çıktıyı (output) ayıklayın\n",
        "    output_only = result.split(input_text)[-1].strip()\n",
        "    final_output = output_only.replace('Response:', '').strip()\n",
        "\n",
        "    # Son olarak, temizlenmiş çıktıyı Excel dosyasının C sütununa yazdırın\n",
        "    sheet[f'C{row}'].value = final_output\n",
        "\n",
        "\n",
        "\n",
        "# Excel dosyasını kaydedin\n",
        "wb.save('testresult.xlsx')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
